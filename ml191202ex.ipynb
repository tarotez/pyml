{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "def list_size(variable, callers_local_vars):\n",
    "    list_size_recursive(' # ', variable, callers_local_vars)\n",
    "\n",
    "def list_size_recursive(header, variable, callers_local_vars):\n",
    "    if type(variable) == list or type(variable) == pandas.core.series.Series:\n",
    "        print(header, 'len(', str([k for k, v in callers_local_vars if v is variable][0]), ') = ', len(variable), sep='')\n",
    "        if(len(variable) > 0):\n",
    "            header += '  '\n",
    "            list_size_recursive(header, variable[0], callers_local_vars)\n",
    "\n",
    "def nparray_size(variable, callers_local_vars):\n",
    "    if(type(variable) == np.ndarray):\n",
    "        print(' # ', str([k for k, v in callers_local_vars if v is variable][0]), '.shape = ', variable.shape, sep='')\n",
    "\n",
    "# shows the name and content of a given variable\n",
    "def see(variable, all=0):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    print('# ', str([k for k, v in callers_local_vars if v is variable][0]), ' (', type(variable), ')', sep='', end='')\n",
    "    if all or (type(variable) != list and type(variable) != np.ndarray and type(variable) != pandas.core.series.Series):\n",
    "        print(' = ', variable, sep='')\n",
    "    else:\n",
    "        print('')\n",
    "    list_size(variable, callers_local_vars)\n",
    "    nparray_size(variable, callers_local_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "colab_type": "code",
    "id": "DRaBeAo2y8rm",
    "outputId": "3b60f0a6-1f20-45c5-97d3-5fa204b92635"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read iris data\n",
    "attrNames = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "\n",
    "dataList = []\n",
    "classLabelsList = []\n",
    "f = open('iris.data', 'r')\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    elems = line.split(',')\n",
    "    if len(elems) > 1:\n",
    "        dataList.append(elems[:-1])\n",
    "        classLabelsList.append(elems[-1])\n",
    "f.close()\n",
    "\n",
    "x = np.float_(np.array(dataList))\n",
    "\n",
    "# convert y to one hot\n",
    "labels = np.unique(classLabelsList)\n",
    "\n",
    "def label2onehot(label):\n",
    "    return np.array([1 if label == elem else 0 for elem in labels])\n",
    "\n",
    "y = np.array([label2onehot(label) for label in classLabelsList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYym_cMTzr2u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# softmax function\n",
    "def softmax(a):\n",
    "    normalizer = np.sum(np.exp(a), axis=1)\n",
    "    return np.array([exp_a / normalizer for exp_a in np.exp(a).transpose()]).transpose()\n",
    "\n",
    "# a single-layer neural network with the softmax function\n",
    "def softmax_nn(x_with_bias, W):\n",
    "    return softmax(np.matmul(W, x_with_bias.transpose()).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a neural network\n",
    "def train(x, y, learning_rate, iterNum):    \n",
    "    x_with_bias = np.c_[x, np.ones(x.shape[0])]\n",
    "    W = np.random.random((y.shape[1], x_with_bias.shape[1]))\n",
    "    loss_dynamics = []\n",
    "    accuracy_dynamics = []\n",
    "    for _ in range(iterNum):\n",
    "        loss = - np.sum(y * np.log(softmax_nn(x_with_bias, W)))\n",
    "        loss_dynamics.append(loss)\n",
    "        preds = np.argmax(softmax_nn(x_with_bias, W), axis=1)\n",
    "        corrects = [1 if p == t else 0 for p, t in zip(preds, np.argmax(y, axis=1))]\n",
    "        accuracy = np.sum(np.array(corrects)) / y.shape[0]\n",
    "        accuracy_dynamics.append(accuracy)        \n",
    "        W = update(x_with_bias, y, W, learning_rate)\n",
    "    return W, loss_dynamics, accuracy_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "iterNum = 500\n",
    "trained_W, loss_dynamics, accuracy_dynamics = train(x, y, learning_rate, iterNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ml191112ex.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
